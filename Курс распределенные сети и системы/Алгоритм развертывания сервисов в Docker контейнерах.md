Запускаем ВМ и подключаемся к ней по [[SSH]] в [[Создание виртуальной машины и установка на нее ОС Linux Debian#Настройка Termius|Termius]] 

![[4_2 Актуальный Файл с командам для занятия v1.0 (1).sh]]

```bash
# Получаем права SuperUser в системе
sudo su
```

Обновляем конфигурацию загрузчика
```bash
# Обновляем конфигурацию загрузчика

# Используем утилиту cat, которая по указателю > перезапишет или создаст файл по пути /etc/default/grub
# <<EOF - концевик файла. Как только cat встретит концевик она запишет содержимое
# После <<EOF начинается содержимое файла которое будет записано

cat > /etc/default/grub <<EOF
```

```ini 
GRUB_DEFAULT=0

# GRUB_TIMEOUT=0  - отключаем таймаут на запись задгузчика, для ускорения запуска ОС
GRUB_TIMEOUT=0 

GRUB_DISTRIBUTOR=`lsb_release -i -s 2> /dev/null || echo Debian`

# GRUB_CMDLINE_LINUX_DEFAULT="quiet text mitigations=off ipv6.disable=1" - включаем использование текстового режима, отключаем ограничения патчей безопасности и отключить ipv6 за ненадобностью, чтобы не нагружать сетевую часть и контейнеры
GRUB_CMDLINE_LINUX_DEFAULT="quiet text mitigations=off ipv6.disable=1"

GRUB_CMDLINE_LINUX=""

GRUB_DISABLE_LINUX_RECOVERY=true

# GRUB_DISABLE_LINUX_RECOVERY=true - отключаем режим восстановления
GRUB_DISABLE_OS_PROBER=true

# GRUB_TERMINAL=console - переводим загрузчик в режим консоли, т.е. по максимуму отключаем грарфический интерфейс, чтобы система потребляла меньше ресурсов
GRUB_TERMINAL=console

EOF
```

Настраиваем систему и перезагружаем, если все ок ^4da079
```bash
# Настраиваем систему и перезагружаем, если все ок

# Оператор && в конце каждой строки определяет, что выполнение следующей команды будет разрешено только при успешном выполнении предыдущей

# Первые две команды записывают параметры переадресации ipv4 и сетевых интерфейсов между собой, чтобы нормально работал vpn  и вообще была переадресация пакетов между интерфейсами
# Командами echo и  >> мы запишем в файл /etc/sysctl.conf значения в ""
echo "net.ipv4.ip_forward=1" >> /etc/sysctl.conf && \
echo "net.ipv4.conf.all.forwarding=1" >> /etc/sysctl.conf && \

# Применяем записанные значения в файле sysctl.conf, чтобы они вступили в работу
sysctl -p /etc/sysctl.conf && \

# systemctl - аналог "служб" в windows, она может управлять запуском и автозапуском приложений
# Отключаем запущенный сервис cron, чтобы не запускать лишние процессы и не занимать оперативную память
systemctl stop cron && \

# Отключаем автозапуск cron, чтобы сервис не запускался при запуске системы
systemctl disable cron && \

# Говорим системе запускаться в режиме multi-user.target - в этом режиме система не ждет запуска графических и некоторых других компонентов, а стартует сразу в текстовом режиме и сообщает другим программам, что мы работаем в текстовом режиме
systemctl set-default multi-user.target && \

# Создаем директорию для хранения ключей. -m 0755 - задаем базовые права
mkdir -m 0755 -p /etc/apt/keyrings && \

# Используем пакетный менеджер apt, который занимается установкой и удалением программ. Через -y передаем флаг yes, чтобы вручную не отвечать на подтверждение действий. Далее перечисляется набор утилит, которые нужно установить 

# **ca-certificates**: Набор доверенных SSL-сертификатов для безопасного взаимодействия с сайтами и сервисами по HTTPS.  

# **curl**: Утилита для передачи данных по различным сетевым протоколам (HTTP, FTP и др.) с поддержкой множества опций.  

# **wget**: Инструмент для загрузки файлов из интернета с возможностью рекурсивного скачивания и продолжения прерванных загрузок.  

# **gnupg/gnupg2**: Реализация PGP (GNU Privacy Guard) для шифрования, подписи данных и управления ключами (gnupg2 — обновлённая версия).  

# **systemd-timesyncd**: Лёгкий сервис синхронизации времени через NTP, встроенный в systemd для поддержания точного системного времени.  

# **htop**: Интерактивный монитор процессов с цветным интерфейсом, управлением процессами и отображением нагрузки на систему.  

# **lm-sensors**: Утилиты для мониторинга температуры, напряжения и скорости вентиляторов через датчики материнской платы и компонентов.  

# **lsb-release**: Скрипт, предоставляющий информацию о дистрибутиве (например, версию и кодовое имя) в соответствии со стандартом LSB.  

# **unzip**: Программа для распаковки архивов в формате ZIP, позволяющая извлекать файлы из сжатых контейнеров.
apt install -y ca-certificates curl wget gnupg gnupg2 systemd-timesyncd htop lm-sensors lsb-release unzip && \

# Загружаем и добавляем ключи и репозитории для Docker. Чтобы далее через пакетный менеджер apt install можно было установить Docker из официального репозитория  
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg && \
echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null && \

# Добавляем репозитории брокера Mosquitto. Не смотря на то, что мы запускаем Mosquitto через Docker, все равно нужно добавлять репозитории, чтобы скачать Mosquitto клиент и можно было из терминала через Mosquitto clients отправить какое-то сообщение в контейнер, т.е. эмулировать работу конечного оборудования  
sudo wget -qO- https://repo.mosquitto.org/debian/mosquitto-repo.gpg.key | gpg --dearmor -o /etc/apt/keyrings/mosquitto-repo.gpg && \
sudo wget -O /etc/apt/sources.list.d/mosquitto-bookworm.list https://repo.mosquitto.org/debian/mosquitto-bookworm.list

# Прогружаем все добавленные репозитории
apt-get update && \

# Устанавливаем Docker и все, что нужно для его работы и утилиту Mosquitto-clients 2.0.20. Через = можно указать конкретную версию для установки
apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin mosquitto-clients=2.0.20-0mosquitto1~bookworm1 && \

# Настраиваем NTP-сервер времени. С помощью sed добавляем строчку в "" в файл .conf. С помощью systemctl restart перезагружаем системного демона синхронизации времени для применения настроек
sed -i 's/#NTP=/NTP=1.ru.pool.ntp.org/' /etc/systemd/timesyncd.conf && \
systemctl restart systemd-timesyncd && \

# Применение изменений в загрузчик GRUB
# Перезагрузка для активации новых параметров ядра
update-grub && \
reboot
```

Создаем скрипт автоматической установки Docker и необходимых файлов конфигураций
```bash
# Создаем скрипт автоматической установки Docker и необходимых файлов конфигураций

# В редакторе nano создаем файл скрита для организации среды для работы Docker-контейнеров
nano install.sh

# Строка `#!/bin/bash` в начале скрипта называется шебанг (shebang). Она указывает интерпретатору системы, какой программой следует выполнить данный скрипт. В данном случае это `/bin/bash`, что означает использование командной оболочки Bash для выполнения скрипта.

#!/bin/bash 

# Получаем IP адрес машины, на которой запускается скрипт, чтобы он автомтически подставился во все конфигурационные файлы, в которых необходим IP-адрес

#В этой строке выполняется следующая последовательность действий:
#1. Команда `ip a` выводит информацию о всех сетевых интерфейсах системы.
#2. Вывод команды `ip a` передаётся через конвейер (`|`) в утилиту `awk`, которая используется для обработки текста.
#3. В `awk` задано условие `/inet / && !/127.0.0.1/`, которое ищет строки, содержащие `inet` (что указывает на IP-адрес), но исключает адрес `127.0.0.1` (локальный адрес loopback).
#4. Для найденной строки выполняется действие `gsub(/\/.*/, "", $2)`, которое удаляет всё после символа `/` в поле `$2` (IP-адрес).
#5. Затем команда `print $2` выводит очищенный IP-адрес.
#6. Команда `exit` завершает работу `awk` после вывода первого найденного IP-адреса.
#7. Результат всей команды присваивается переменной `vm_ip` с помощью конструкции `vm_ip=$(...)`.
#Таким образом, эта строка получает IP-адрес машины, на которой запускается скрипт, исключая локальный адрес `127.0.0.1`.
vm_ip=$(ip a | awk '/inet / && !/127.0.0.1/ {gsub(/\/.*/, "", $2); print $2; exit}')  

#Выводим информацию о Docker и IP
docker compose version && \
docker -v && \
echo "IP Машины: "$vm_ip

#Проверяем, является ли пользователь root'ом
if [ "$EUID" -eq 0 ]; then
    echo "Этот скрипт не должен запускаться с правами root. Запустите его от имени обычного пользователя."
    exit 1
fi  

# Получаем имя текущего пользователя
# Знак `$` в скриптах используется для обращения к значениям переменных.
current_user=$USER  

# Создаем необходимые каталоги для docker-compose.yml

# YML-файлы очень привередливы к пробелам и отступам

#Строка `user_home=$(eval echo ~$current_user)` выполняет следующее:
#1. Переменная `$current_user` содержит имя текущего пользователя.
#2. Команда `echo ~$current_user` выводит домашний каталог текущего пользователя (например, `/home/username`).
#3. Команда `eval` используется для выполнения результата команды `echo`, что позволяет получить путь к домашнему каталогу пользователя в виде строки.
#4. Результат присваивается переменной `user_home`.
#Строка `mkdir -p $user_home` выполняет следующее:
#1. Команда `mkdir -p` создаёт каталог, указанный в аргументе, если он ещё не существует.
#2. Флаг `-p` гарантирует, что команда не будет выдавать ошибку, если каталог уже существует, и позволяет создавать вложенные каталоги.
user_home=$(eval echo ~$current_user)
mkdir -p $user_home

# Создаем переменные с уникальными портами для пользователя, чтобы далее вместо цифр оперировать осмысленными названиями. Номера портов - дефолтные для каждого из сервисов. Но в принципе можно использовать любой номер в диапазоне от 1024 до 65535
mosquitto_port=1883
influxdb_port=8086
grafana_port=3000
nodered_port=1880
wireguard_port=51820
telegraf_port1=8092
telegraf_port2=8094
telegraf_port3=8125

# Создаем docker-compose.yml для текущего пользователя

# Файл `docker-compose.yml` необходим для определения и управления мультиконтейнерными приложениями Docker. Он позволяет описать сервисы, их зависимости, настройки сети, тома и другие параметры в одном файле. Это упрощает процесс развёртывания и управления несколькими контейнерами как единым целым.
# С помощью `docker-compose.yml` можно легко запустить все необходимые контейнеры с правильно настроенными связями между ними, что особенно полезно при разработке и тестировании приложений, состоящих из нескольких сервисов.

# Вместо переменных будут подставлены необходимые нам значения 
cat > "$user_home/docker-compose.yml" << EOF
```

Содержимое `docker-compose.yml` ^975290
```yaml
services:
  mosquitto:
  
# используется образ Mosquitto для Docker. Названия контейнеров можно найти на Docker Hub
    image: eclipse-mosquitto
    
# настройка сетевого режима, чтобы мы могли обращаться к сервису просто по указанию порта
    network_mode: bridge
    
# запуск контейнера от имени текущего пользователя
    user: $(id -u $current_user):$(id -g $current_user)
    
# отображение портов контейнера на хост-машине, т.е. название порта, по которому будет доступен сервис. Мэппинг 1 к 1 т.е. то что мы указали в значении переменной mosquitto_port соответствует номеру порта 1883.
    ports:
      - "$mosquitto_port:1883"

# монтирование директорий хост-машины в контейнер для сохранения данных, чтобы сохранять нужные данные и конфигурации при перезапуске контейнера, т.е. мы ставим в соответствие адреса каталогов на хосте адресам каталогов ВМ
    volumes:
      - "$user_home/mosquitto/config:/mosquitto/config"
      - "$user_home/mosquitto/data:/mosquitto/data"
      - "$user_home/mosquitto/log:/mosquitto/log"

# настройка переменной окружения `TZ` для корректного отображения времени
    environment:
      - TZ=Europe/Moscow

# автоматический перезапуск контейнера при сбоях
    restart: unless-stopped


  telegraf:
# На Docker Hub можно выбрать теги, которые мы можем указать для image для конткретного приложения
    image: telegraf:alpine
    network_mode: bridge
    user: $(id -u $current_user):$(id -g $current_user)
    
# монтирование директорий хост-машины в контейнер для сохранения конфигураций и данных
    volumes:
      - "$user_home/telegraf/conf:/etc/telegraf/telegraf.d"
    ports:
      - "$telegraf_port1:8092/tcp"
      - "$telegraf_port1:8092/udp"
      - "$telegraf_port2:8094/tcp"
      - "$telegraf_port2:8094/udp"
      - "$telegraf_port3:8125/tcp"
      - "$telegraf_port3:8125/udp"
    environment:
      - TZ=Europe/Moscow
      
# зависимость от сервисов Mosquitto и InfluxDB. Она сообщает Docker Compose, что перед запуском определённого сервиса необходимо убедиться, что другие указанные сервисы уже запущены и работают. Это особенно полезно в случаях, когда один сервис зависит от корректной работы другого для своего функционирования.
    depends_on:
      - mosquitto
      - influxdb
    restart: unless-stopped

  influxdb:
# используется образ InfluxDB для Docker,развёрнутый на минималистичной операционной системе Alpine Linux. Alpine Linux известна своим небольшим размером и эффективностью, что делает её популярным выбором для контейнеров Docker. Использование такого образа позволяет запустить InfluxDB в легковесной среде, что экономит ресурсы и ускоряет запуск контейнер
    image: influxdb:alpine
    network_mode: bridge
    ports:
      - "$influxdb_port:8086"
    volumes:
      - "$user_home/influxdb/data:/var/lib/influxdb2"
      - "$user_home/influxdb/conf:/etc/influxdb"
      - "$user_home/influxdb/engine:/var/lib/influxdb2/engine"
    environment:
      - TZ=Europe/Moscow
    restart: unless-stopped

  grafana:
    image: grafana/grafana
    network_mode: bridge
    user: $(id -u $current_user):$(id -g $current_user)
    volumes:
      - "$user_home/grafana/data:/var/lib/grafana"
      - "$user_home/grafana/conf:/etc/grafana"
      - "$user_home/grafana/log:/var/log/grafana"
    ports:
      - "$grafana_port:3000"
    environment:
# Переменные окружения `PUID` и `PGID` используются для указания идентификаторов пользователя и группы в контейнерах Docker. Они позволяют сопоставить идентификаторы пользователя и группы внутри контейнера с соответствующими идентификаторами на хост-системе. Это обеспечивает правильное владение файлами и папками, а также предотвращает проблемы с разрешениями при обмене данными между контейнером и хостом.
      - PUID=$(id -u $current_user)
      - PGID=$(id -g $current_user)
      - TZ=Europe/Moscow
    depends_on:
      - telegraf
    restart: unless-stopped

  nodered:
  
# Это образ Docker, который содержит последнюю минималистичную версию Node-RED. Добавление слова `latest-minimal` в названии образа указывает на то, что используется самая свежая версия с минимальным набором компонентов, что делает образ более лёгким и быстрым в развёртывании.
    image: nodered/node-red:latest-minimal
    network_mode: bridge
    user: $(id -u $current_user):$(id -g $current_user)
    volumes:
      - "$user_home/node-red/data:/data"
    ports:
      - "$nodered_port:1880"
    environment:
      - TZ=Europe/Moscow
    depends_on:
      - telegraf
    restart: unless-stopped
    
  wireguard:
  
# Это образ Docker, который содержит последнюю версию WireGuard, популярного VPN-решения. Образ размещён на репозитории lscr.io и предназначен для использования с Linux-серверами. Этот образ позволяет легко развернуть WireGuard в контейнере Docker, что упрощает настройку и управление VPN-сервером.
    image: lscr.io/linuxserver/wireguard:latest
    network_mode: bridge
    
# Предоставление привилегий контейнеру для работы с сетевыми настройками.
    privileged: true
    volumes:
      - "$user_home/wireguard/config:/config"
    ports:
      - "$wireguard_port:51820/tcp"
      - "$wireguard_port:51820/udp"
    environment:
#`PUID` — идентификатор пользователя (User ID) в системе хоста, под которым будет работать WireGuard внутри контейнера. Это позволяет сопоставить пользователя внутри контейнера с реальным пользователем на хост-системе.
      - PUID=$(id -u $current_user)

#`PGID` — идентификатор группы (Group ID) в системе хоста, к которой принадлежит пользователь, работающий внутри контейнера. Аналогично `PUID`, это обеспечивает правильное владение файлами и папками.
      - PGID=$(id -g $current_user)

#`TZ` — настройка часового пояса. Позволяет установить правильный часовой пояс для работы WireGuard, что важно для корректного отображения времени в логах и других данных.
      - TZ=Europe/Moscow

# `SERVERURL` — опциональный параметр, который задаёт URL сервера WireGuard. Может быть полезен для настройки определённых параметров подключения.
# Далее нужно будет заменить на белый IP-адрес
      - SERVERURL=$vm_ip #optional

# `SERVERPORT` — опциональный параметр, указывающий порт, на котором будет работать сервер WireGuard.
      - SERVERPORT=$wireguard_port #optional

# `PEERS` — опциональный параметр, определяющий количество пиров (peer), которые могут подключиться к серверу WireGuard.
      - PEERS=10 #optional

# `PEERDNS` — опциональный параметр для настройки DNS-серверов, которые будут использоваться пирами при подключении к VPN.
      - PEERDNS=1.1.1.1 #optional

# `INTERNAL_SUBNET` — опциональный параметр, задающий внутреннюю подсеть для VPN. Это диапазон IP-адресов, которые будут использоваться внутри VPN-сети.
      - INTERNAL_SUBNET=10.13.13.0 #optional

# `ALLOWEDIPS` — опциональный параметр, указывающий разрешённые IP-адреса или подсети для подключения к VPN.
      - ALLOWEDIPS=0.0.0.0/0 #optional

#`LOG_CONFS` — опциональный параметр, который включает или отключает логирование конфигураций. По умолчанию установлено значение `false`, что отключает логирование
      - LOG_CONFS=false #optional

# `cap_add` — это параметр в файле `docker-compose.yml`, который позволяет добавить дополнительные возможности (capabilities) контейнеру Docker. Возможности — это механизм в Linux, который предоставляет процессам определённые привилегии, обычно доступные только системному администратору.
# Добавление возможностей контейнеру необходимо в случаях, когда приложению внутри контейнера требуются расширенные права для выполнения определённых задач, например, работы с сетевыми настройками или модулями ядра Linux.
# В контексте WireGuard, например, добавление возможностей `NET_ADMIN` и `SYS_MODULE` позволяет контейнеру выполнять операции, связанные с настройкой сети и управлением модулями ядра, что необходимо для корректной работы VPN-сервера
    cap_add:

# - `NET_ADMIN` позволяет контейнеру выполнять операции, связанные с настройкой сетевых интерфейсов, маршрутизации и других параметров сети. Это необходимо для работы приложений, которые требуют доступа к низкоуровневым сетевым функциям, например, для настройки VPN-серверов или других сетевых сервисов.
      - NET_ADMIN

# `SYS_MODULE` предоставляет контейнеру возможность загружать и выгружать модули ядра Linux. Это может быть необходимо для работы некоторых приложений, которые зависят от специфических модулей ядра или требуют динамической загрузки модулей во время выполнения
      - SYS_MODULE
# `sysctls` — это параметр в файле `docker-compose.yml`, который позволяет настраивать параметры ядра Linux внутри Docker-контейнера. Эти параметры управляют поведением системы на низком уровне и могут влиять на различные аспекты работы, такие как сетевая подсистема, управление памятью и т. д.
    sysctls:

# параметр `net.ipv4.conf.all.src_valid_mark=1` относится к настройке сетевой подсистемы. Он устанавливает значение переменной sysctl, которая контролирует поведение IP-стека ядра Linux. В данном случае `net.ipv4.conf.all.src_valid_mark=1` позволяет маркерам валидации источника (source validation marks) работать на всех сетевых интерфейсах. Это может быть полезно для настройки определённых сетевых функций или для обеспечения дополнительной безопасности в сетевых взаимодействиях.
      - net.ipv4.conf.all.src_valid_mark=1

    restart: unless-stopped

EOF
```

Продолжение файла `nano install.sh`
```bash
# Создаем необходимые каталоги с разрешениями только для текущего пользователя, на которые мы ссылались в `docker-compose.yml`
mkdir -m 755 -p $user_home/mosquitto/config
mkdir -m 755 -p $user_home/mosquitto/data
mkdir -m 755 -p $user_home/mosquitto/log
mkdir -m 755 -p $user_home/influxdb/data
mkdir -m 755 -p $user_home/influxdb/conf
mkdir -m 755 -p $user_home/influxdb/engine
mkdir -m 755 -p $user_home/telegraf/conf
mkdir -m 755 -p $user_home/grafana/data
mkdir -m 755 -p $user_home/grafana/conf
mkdir -m 755 -p $user_home/grafana/log
mkdir -m 755 -p $user_home/node-red/data
mkdir -m 755 -p $user_home/wireguard/config

# Создание конфигурационных файлов для созданного docker-compose.yml
# Они необходимы для первого запуска контейнеров

# Создаем файлы конфигурации для Mosquitto для текущего пользователя
cat > "$user_home/mosquitto/config/mosquitto.conf" << EOF
```

Содержимое `mosquitto.conf`
```ini
# Если мы указываем здесь другой порт, то надо поменять на аналогичный порты, кторые прописаны в `docker-compose.yml`
listener 1883
EOF
```

Продолжение файла `nano install.sh`
```bash
# Создаем файлы конфигурации для Grafana для текущего пользователя
cat > "$user_home/grafana/conf/grafana.ini" << EOF
```

Содержимое `grafana.ini`
```ini
# Фактически это файл заглушка, который при необходимости можно использовать, например поменять название порта, используемого Grafana, аналогично надо будет внести изменения в `docker-compose.yml`
[server]
;http_port = 3000
EOF
```

Продолжение файла `nano install.sh`
```bash
# Создаем файлы конфигурации для Telegraf для текущего пользователя
cat > "$user_home/telegraf/conf/telegraf.conf" << EOF
```

Содержимое `telegraf.conf`
```ini
# Конфигурация агента telegraf
# [agent] — настройки самого агента Telegraf:
[agent]

# интервал сбора метрик, в данном случае каждые 3 секунды. При настройке параметра `interval` важно найти баланс между детализацией мониторинга и нагрузкой на систему, учитывая конкретные требования и ресурсы вашей инфраструктуры.
  interval = "3s"
  
# округление времени сбора до ближайшего интервала
  round_interval = true
  
# размер пакета метрик для отправки
  metric_batch_size = 1000
  
# лимит буфера для хранения метрик 
  metric_buffer_limit = 10000

# случайное отклонение интервала сбора, здесь установлено 0 секунд
  collection_jitter = "0s"
  
# интервал сброса буфера метрик
  flush_interval = "3s"
  
# случайное отклонение интервала сброса, здесь 0 секунд
  flush_jitter = "0s"
  
# точность временных штампов, по умолчанию пустая строка
  precision = ""
  
# имя хоста, по умолчанию пустое
  hostname = ""
  
# флаг, указывающий, следует ли включать имя хоста в метрики
  omit_hostname = false
  
# настройки для вывода данных в InfluxDB v2
[[outputs.influxdb_v2]]

# URL сервера InfluxDB, где `$vm_ip` — IP-адрес машины, а `$influxdb_port` — порт InfluxDB
  urls = ["http://$vm_ip:$influxdb_port"]

# токен для аутентификации в InfluxDB, здесь оставлен пустым. Токен появится позже
  token = " "

# организация в InfluxDB
  organization = "IoT"

# бакет (хранилище) в InfluxDB для хранения данных
  bucket = "IoT"

# настройки для потребления данных из MQTT
[[inputs.mqtt_consumer]]

# сервер MQTT, где `$mosquitto_port` — порт Mosquitto
  servers = ["tcp://$vm_ip:$mosquitto_port"]

# топики MQTT для подписки, здесь используется wildcard `#` для подписки на все топики
  topics = ["#"]

# имя пользователя для аутентификации на сервере MQTT
  username = "IoT"

# пароль для аутентификации
  password = "student"

# формат данных, получаемых из MQTT
  data_format = "value"

# тип данных, получаемых из MQTT, в данном случае float
  data_type = "float"

EOF
```

Продолжение файла `nano install.sh`
``` bash
# Создаем скрипт для настройки пользовательских параметров
cat > "$user_home/settings.sh" << EOF
```

Содержимое файла `settings.sh`
```bash
#!/bin/bash

# Mosquitto
# Устанавливаем имя пользователя и пароль для Mosquitto
echo "Настройка Mosquitto. Создание пользователя IoT и пароля:"
# Эта команда запрашивает у пользователя ввод пароля для учётной записи IoT. Флаг `-s` (secure) скрывает введённые символы, чтобы предотвратить их отображение на экране, а флаг `-p` позволяет вывести подсказку перед вводом. Введённый пароль сохраняется в переменную `password_mosquitto`.
read -s -p "Введите пароль для пользователя IoT: " password_mosquitto

# Эта команда выводит пустую строку, чтобы перейти на новую строку после ввода пароля, что улучшает читаемость и удобство интерфейса
echo

# эта команда выполняет следующее:
# запускает интерактивный сеанс в контейнере Docker с именем `$current_user-mosquitto-1`;
# использует утилиту `mosquitto_passwd` для создания файла с хешами паролей (`/mosquitto/config/password.txt`) для пользователя IoT;
# флаг `-c` указывает на создание нового файла паролей.
docker exec -it $current_user-mosquitto-1 mosquitto_passwd -c /mosquitto/config/password.txt IoT

#эта команда:
# использует утилиту `sed` для редактирования файла конфигурации Telegraf (`~/telegraf/conf/telegraf.conf`);
# выполняет поиск строки, содержащей `password =`, и заменяет её на `password = "$password_mosquitto"`, где `$password_mosquitto` — это переменная, содержащая пароль, введённый пользователем ранее;
# флаг `-i` указывает на редактирование файла на месте, без создания временной копии.
sed -i "s/password = .*/password = \"\$password_mosquitto\"/" ~/telegraf/conf/telegraf.conf
# Создаем файл конфигурации Mosquitto
cat > $user_home/mosquitto/config/mosquitto.conf << EOSF
```

Cодержимое файла `mosquitto.conf`
```ini
# эта команда указывает, что сервер MQTT будет слушать подключения на порту 1883. Это стандартный порт для незашифрованных соединений MQTT.
listener 1883

# данная настройка запрещает анонимный доступ к серверу MQTT. Это означает, что для подключения к серверу пользователям необходимо будет пройти аутентификацию, предоставив корректные учётные данные.
allow_anonymous false

# эта строка указывает путь к файлу, содержащему пароли пользователей. В данном случае файл называется `password.txt` и находится в каталоге `/mosquitto/config`. Сервер MQTT будет использовать этот файл для проверки подлинности пользователей при попытке подключения.
password_file /mosquitto/config/password.txt

# Т.к. мы создаем скрипт в скрипте, то используем другой концевик файла, чтобы все работало корректно
EOSF
```

Продолжение файла `settings.sh`
```bash
# Перезапускаем контейнер Mosquitto
docker restart $current_user-mosquitto-1

  
# Node-red

# Запускаем скрипт nodered-docker-folder.sh и инициализируем Node-red

echo "Настройка Node-red"

# эта команда выполняет следующие действия:
# Запускается интерактивный сеанс в контейнере Docker с именем `$current_user-nodered-1` с правами пользователя root.
# Изменяются UID (идентификатор пользователя) для пользователя `node-red` в файлах `/etc/passwd` и `/etc/group` на значение `1100`.
docker exec -it --user=root $current_user-nodered-1 sh -c 'sed -i "s/^node-red:x:[0-9]*/node-red:x:1100/" /etc/passwd && sed -i "s/^node-red:x:[0-9]*:/node-red:x:1100:/" /etc/group'

# эта команда выполняет следующие действия:
# Проверяется наличие пользователя `$current_user` в файлах `/etc/group` и `/etc/passwd`. Если пользователя нет, он добавляется.
# Создаётся домашний каталог для пользователя `$current_user` по пути `/home/$current_user`.
# Меняются владелец и права доступа для домашнего каталога на `$current_user`.
docker exec -it --user=root $current_user-nodered-1 sh -c 'if ! grep -q "^$current_user:" /etc/group; then echo "$current_user:x:$(id -u $current_user):" >> /etc/group; fi && if ! grep -q "^$current_user:" /etc/passwd; then echo "$current_user:x:$(id -u $current_user):$(id -g $current_user)::/home/$current_user:/bin/ash" >> /etc/passwd; fi && mkdir -p /home/$current_user && chown $current_user:$current_user /home/$current_user && chmod 755 /home/$current_user'

# эта команда запускает инициализацию Node-RED внутри контейнера Docker с именем `$current_user-nodered-1`
docker exec -it $current_user-nodered-1 node-red-admin init

# Копируем файл настроек Node-red в другой каталог, для сохранения на жестком диске 
docker exec $current_user-nodered-1 cat ~/.node-red/settings.js > ~/node-red/data/settings.js  

# Перезапускаем контейнер Node-red
docker restart $current_user-nodered-1


# InfluxDB

# Запрашиваем у пользователя имя пользователя и пароль для InfluxDB
echo "Настройка InfluxDB"
read -p "Введите имя пользователя InfluxDB: " influx_user
read -s -p "Введите пароль для InfluxDB: " influx_password
echo  

# Настраиваем базу данных InfluxDB с предоставленными данными
# 1. `echo -e "0\ny"` — генерирует последовательность ответов «0» и «y», которые будут подаваться на ввод для автоматизации взаимодействия с командной строкой.
# 2. `docker exec -i $current_user-influxdb-1` — запускает интерактивный сеанс в контейнере Docker с именем `$current_user-influxdb-1`.
# 3. `influx setup` — запускает процесс настройки InfluxDB.
# 4. Параметры настройки:
# `-b IoT` — создаёт базу данных с именем IoT.
# `-u "$influx_user"` — указывает имя пользователя для доступа к базе данных.
# `-p "$influx_password"` — указывает пароль для пользователя.
# `-o IoT` — создаёт организацию с именем IoT.
# Таким образом, команда полностью настраивает InfluxDB с заданными параметрами, используя предопределённые ответы для интерактивных запросов.
echo -e "0\ny" | docker exec -i $current_user-influxdb-1 influx setup -b IoT -u "\$influx_user" -p "\$influx_password" -o IoT

# Команда создаёт токен авторизации для организации IoT в InfluxDB и сохраняет его в переменную `token`:
# 1. `docker exec -it $current_user-influxdb-1` — запускает интерактивный сеанс в контейнере Docker с именем `$current_user-influxdb-1`.
# 2. `influx auth create` — создаёт новый токен авторизации в InfluxDB.
# 3. Параметры создания токена:
# `--org IoT` — указывает организацию, для которой создаётся токен.
# `--description "$(id -un) token"` — устанавливает описание токена, включающее имя текущего пользователя.
# `--operator` — указывает, что токен предоставляет права оператора.
# 4. `awk 'NR > 1 {print $4}'` — использует awk для извлечения четвёртого поля (токена) из вывода команды, начиная со второй строки.
token=\$(docker exec -it $current_user-influxdb-1 influx auth create --org IoT --description "\$(id -un) token" --operator | awk 'NR > 1 {print \$4}')

# Выводим токен пользователю
# Токен нужен для того чтобы, с правами, которые созданы командой выше, указывая токен во внешних программах эти программы могли взаимодействовать с InfluxDB (в нашем случае Node-RED и Grafana)
echo "Токен доступа: \$token"

# Записываем переменную 'token' в файл, чтобы не потерять
echo "\$token" > token.txt

# Заменяем значение токена в файле telegraf.conf, в поле token, которое мы выше при конфигурации оставили пустым
sed -i 's/token =.*/token = "'"\$token"'" /' ~/telegraf/conf/telegraf.conf

# Перезапускаем контейнер Telegraf
docker restart $current_user-telegraf-1

EOF
```

Окончание файла `nano install.sh`
```bash
# Устанавливаем владельца и разрешения для созданных файлов конфигурации и задаем права, чтобы не было проблем с доступом у самого Docker'а
# `chown $current_user:$current_user "$user_home/docker-compose.yml"` и аналогичные команды для других файлов — меняют владельца и группу файлов на `$current_user`. Это означает, что файлы передаются в собственность текущего пользователя, что позволяет ему иметь необходимые права на доступ и изменение этих файлов.
chown $current_user:$current_user "$user_home/docker-compose.yml"
chown $current_user:$current_user "$user_home/mosquitto/config/mosquitto.conf"
chown $current_user:$current_user "$user_home/grafana/conf/grafana.ini"
chown $current_user:$current_user "$user_home/telegraf/conf/telegraf.conf"
chown $current_user:$current_user "$user_home/settings.sh"

# 1. `chmod 755 "$user_home/docker-compose.yml"` и аналогичные команды для других файлов — устанавливают права доступа к файлам. Значение `755` означает:
# владелец файла имеет права на чтение (`r`), запись (`w`) и выполнение (`x`);
# группа владельца и другие пользователи имеют права только на чтение и выполнение файла.
chmod 755 "$user_home/docker-compose.yml"
chmod 755 "$user_home/mosquitto/config/mosquitto.conf"
chmod 755 "$user_home/grafana/conf/grafana.ini"
chmod 755 "$user_home/telegraf/conf/telegraf.conf"
chmod 755 "$user_home/settings.sh"

# Проверяем, является ли текущий пользователь уже частью группы Docker
# команда проверяет, является ли пользователь `$current_user` членом группы Docker. Команда `groups` выводит список групп, к которым принадлежит пользователь, а `grep -q '\bdocker\b'` ищет в этом списке строку «docker»
if groups $current_user | grep -q '\bdocker\b'; then

# если пользователь уже является членом группы Docker, выполняется условие `then`, и выводится сообщение: `echo`
  echo "Пользователь '$current_user' уже частью группы Docker."

else

# добавляет пользователя `$current_user` в группу Docker.
  sudo usermod -aG docker $current_user

  echo "Пользователь '$current_user' добавлен в группу Docker."

# Команда `newgrp docker` используется для того, чтобы текущий пользователь начал использовать группу Docker в текущем сеансе без необходимости перезапуска терминала или сеанса. Это позволяет немедленно применить изменения группы, что может быть необходимо для получения нужных прав доступа к ресурсам, управляемым Docker.
  newgrp docker

fi
```


```bash
# запуск скрипта `install.sh`, который выполняет различные настройки и создаёт необходимые файлы конфигурации. Нужно будет ввести пароль от sudo для того, чтобы добавить текущего пользователя в группу Docker
bash install.sh

# команда запускает сервисы, описанные в файле `docker-compose.yml`, в фоновом режиме. После развертывания контейнеров, выполнив команду `docker ps`, мы увидим что сервис Telegraf в ошибке. Это произошло из-за того, что в его конфигурации пока не указан токен доступа к IfuxDB.

Для завершения процесса выполним  конфигурацию базовых компонентов
docker compose up -d

# запуск скрипта `settings.sh`, который настраивает параметры для различных сервисов, таких как Mosquitto, Node-RED и InfluxDB. 

# Mosquitto
# Для конфигурации Mosquitto необходимо трижды ввести один и тот же пароль (например student). Вводить нужно внимательно иначе придется вручную править конфигурационные файлы. 

# Node-RED:
# расположение файла - по умолчанию
# конфигурация пользователей 
# username (admin) - пароль(student1) - полные права на доступ
# создать еще одного пользователя 
# username (student) - пароль(student1) - права - только чтение
# далее отказывемся либо настройки по умолчанию
# далее вводим кодовое слово для ширфования файла с credential`ами
# далее настройки по умолчанию
# разрешить использование внешних модулей - да

# InfluxDB
# имя пользователя (admin) - пароль (student1)
# скопировать токен доступа
bash settings.sh

# команда останавливает и удаляет контейнеры, сети и образы, связанные с проектом Docker Compose
docker compose down

# повторный запуск сервисов в фоновом режиме после их остановки. Если контейнеры не были раннее установлены, то произойдет загрузка Image(образов) приложения из DockerHub, и из этих образов будут запущены эти контейнеры из которых будут работать сервисы 
docker compose up -d

# команда выводит список текущих работающих контейнеров Docker
docker ps
```
